---
id: 829fb
title: 'Bemoaning AI won’t stop it, but f**k me, we need to be careful.'
date: '2026-01-18 01:12:03'
slug: bemoaning-ai-wont-stop-it-but-fk-me-we-need-to-be-careful
excerpt: 'AI and LLMs are likely here to stay, at least until the bubble bursts and we have to figure out what parts will survive, but a recent uptick in influencers and industry thought leaders pushing AI content has me worried.'
---

Disclaimer: I use AI *every* day, I use claude code to whip up little app ideas i’ve had, and/or write tests, write proof of concept features etc, we’re using it in PR reviews and it IS catching things, and we’re seeking to use agents more, we’re a small development team of 5, and using it to **augment** rather than replace anybody in our team, long may that be the case.

I'm not writing this to scream "nobody should use AI", but, as the title suggests, i'm worried about over-reliance, and people being taken advantage of by opportunists through deliberaly making people feel like they're falling behind.

## Affordability

Firstly, I need to say this out loud: _not everyone can afford to learn to use AI tools yet_. It is simply not a democratised tool in the way that most of the software we use is today.

$200 a month for a Claude Max plan is an insane amount of money to someone who could be using that for rent or food (both of which cost more than ever, right now).

And you might say “you don’t need that”, but if you spend 10 minutes listening to any current online discourse around AI ‘development’, it’s all about “token maxxing”, many people are running Claude, OpenAI Codex, and Google Gemini plans alongside each other, spending hundreds.

> The current suggestion is, simply, that if you’re not doing this level of “pushing it to the limit” with as much token usage as possible, you’re falling behind.

Software development is, or used to be, something that was - effectively - *free* to learn, as long as you had a laptop. Or at the very least you could choose where best to invest the limited cash you had.

Making being comfortable, competent, or an “expert” at using AI development tools as a requirement for jobs raises the barrier for entry in not only an extreme way, but also in the WRONG way - what happens when those tools are unavailable, when you’re priced out, or when an incident occurs with a tool that is poorly understood by AI? We’re abandoning fundamental skills in programming for a perceived increase in productivity.

## Patterns of Grift.

We’re seeing more people who used to valuable contributors pivot more towards pushing AI rhetoric, and the inevitible monetisation of the hype.

The “grift” cycle is well defined, and a known quantity. I know that Grift in software development is nothing new, we’ve been putting up with people doing stupid things like:

- Folk selling Node SaaS “starter” kits that lack basic security to people who don’t know enough to be able to see that as a problem.
- Normalising and even celebrating that guy who created a Therapist AI, and was praised by multitudes for his quick-thinking and “entrepenurial spirit”, while ignoring many who said it was unethical, extremely dangerous, and would lead to disaster.

The pattern of grift is well documented:

**Early phase**: Influencers with existing audiences pivot hard to the new technology, positioning themselves as experts despite often having surface-level understanding. They frame it as a revolutionary opportunity that will leave skeptics behind.

**Monetization phase**: They launch courses, paid communities, consultancies, or NFT/token projects themselves. The pitch is always "I'm sharing my secrets to help YOU get in early" while they're actually extracting value from their audience's FOMO. Revenue comes from selling picks and shovels, not from the underlying tech.

**Maximalist rhetoric**: Everything becomes about this one technology. They dismiss critics as "not getting it" or being too risk-averse. Past tech skepticism is reframed as historical mistakes ("imagine not buying Bitcoin in 2010!").

**Exit phase**: When the bubble shows cracks, they either quietly pivot to the next thing, claim they "always said to be careful," or double down briefly before disappearing from that topic entirely. Courses get quietly delisted, tweets get deleted.

The fact is, we’re currently seeing this. And with some people whom I really admired, too. I'm not calling everyone doing this a "grifter", but they have to be careful with their messaging and rhetoric. The key things there being:

- “They frame it as a revolutionary opportunity that will leave skeptics behind.”
- “Everything becomes about this one technology.”
- “They dismiss critics as "not getting it" or being too risk-averse”

All of these talking points are already happening _heavily_, and it’s concerning, because the message it’s sending to people who *don’t* work this way (and maybe don't want to) is that they’re somehow falling behind, when the fact is that you can’t truly fall behind on something that is so bleeding edge, and changing so quickly, that it has no set standards yet.  Driving FOMO is such a horrid tactic to gain customers.

Opportunists in all industries will always be opportunists, there’s nothing we can do there, and nobody disagrees that you’ve gotta put food on the table somehow, but please, have some measure of caution, and some awareness of the impact of pushing this message of “you’re falling behind” to people in the industry.

## AI is designed to take your job.

Bold statement, but I challenge you to prove me wrong at a fundamental level.

They’re building tools that can do things people can do in a fraction of their time and at a fraction of the cost, with no moral safeguards, how is that not directly meant to appeal to companies that want to maximise profits and reduce risk?

AI is, *fundamentally*, a capitalist venture. It’s aim is not to make the world a better place, or to build a cool new technology, it’s to make people MORE money.

In fact, it might be the *most* definitive capitalist venture ever - it wants to replace people with machines to increase profits and reduce risk.

- [Exclusive: OpenAI to remove non-profit control and give Sam Altman equity](https://www.reuters.com/technology/artificial-intelligence/openai-remove-non-profit-control-give-sam-altman-equity-sources-say-2024-09-25/)
- [Sam Altman warns AI could kill us all. But he still wants the world to use it | CNN Business](https://edition.cnn.com/2023/10/31/tech/sam-altman-ai-risk-taker)

Remember that every piece of code you ask AI to scan, it learns from, and is trained on, learning to better replace you. They want to **foster complete dependence** on this tool without teaching you how anything works behind the scenes. Every coding session feeds it the information it needs to improve its ability to replace you.

## Cannibalisation.

The eventual outcome of AI being both the main consumer of code and the main producer of code is that it will begin to effectively eat it's own shit. Once the volume of code produced by AI surpasses the code produced by humans, we'll suffer from a distinct lack of innovation.

Not just that, but we'll soon see the advent of frameworks and potentially languages that are designed entirely for AI coding, with no need for human readability and eschewing developer experience for machine readability and token efficiency, further increasing reliance on these systems.

And there are plenty of people talking, and even being comfortable with the fact that we’re going to lose the traditional “Junior Developer”.

I’m not sure about you all, but I learned _so_ much from other developers when I was a junior - far more than I did from reading books and stack overflow, that’s for sure. Getting rid of this relationship to save some money by replacing Juniors with AI absolute sucks as a concept, but also the practicality is it will severely limit hiring options in the future, and again, massively increses dependency on these technologies.

Maybe AI could be the solution as well: AI would become the new mentor of Junior Developers, but then where does that leave senior developers, if it can write better than us AND has the soft skills.

## The obvious environmental and ethical concerns.

There are serious ethical and environmental concerns behind using products from these companies, too.

AI’s expanding power usage - [We did the math on AI’s energy footprint. Here’s the story you haven’t heard.](https://www.technologyreview.com/2025/05/20/1116327/ai-energy-usage-climate-footprint-big-tech/)

AI’s expanding CO2 emissions - [AI boom has caused same CO2 emissions in 2025 as New York City, report claims](https://www.theguardian.com/technology/2025/dec/18/2025-ai-boom-huge-co2-emissions-use-water-research-finds)

An OpenAI whistleblower reportedly killed himself - [Suchir Balaji: OpenAI whistleblower found dead in apartment](https://www.bbc.co.uk/news/articles/cd0el3r2nlko).

Anthropic’s CEO is okay with enriching dictators (his own words) to take investment money from qatar - [Leaked Memo: Anthropic CEO Says the Company Will Pursue Gulf State Investments After All](https://www.wired.com/story/anthropic-dario-amodei-gulf-state-leaked-memo/)

Open AI’s lack (removal?) of safeguards led to a suicide of a teen:

- [OpenAI denies allegations that ChatGPT is to blame for a teenager's suicide](https://www.nbcnews.com/tech/tech-news/openai-denies-allegation-chatgpt-teenagers-death-adam-raine-lawsuit-rcna245946)
- https://time.com/7327946/chatgpt-openai-suicide-adam-raine-lawsuit/
- https://www.cbsnews.com/news/chatgpt-lawsuit-colordo-man-suicide-openai-sam-altman/

Grok’s well documented ~DISGUSTING~ ability to remove clothes from anyone just by asking it, including **children**.

- https://www.engadget.com/ai/elon-musks-grok-ai-posted-csam-image-following-safeguard-lapses-140521454.html
- [IWF finds sexual imagery of children which 'appears to have been' made by Grok](https://www.bbc.co.uk/news/articles/cvg1mzlryxeo)
- https://www.theguardian.com/technology/2026/jan/02/elon-musk-grok-ai-children-photos

Grok’s datacentre is pumping tonnes of harmful Nitrogen Oxide into the air illegally at a memphis datacentre to meet it’s power needs - [Elon Musk’s xAI datacenter generating extra electricity illegally, regulator rules](https://www.theguardian.com/technology/2026/jan/15/elon-musk-xai-datacenter-memphis)

Where are the regulations, guidelines, and consequences for these things?

In our own Laravel space, we’ve seen layoffs at [Laracasts](https://bsky.app/profile/simonswiss.com/post/3mbsh4tepyk2a) and [TailwindCSS](https://github.com/tailwindlabs/tailwindcss.com/pull/2388#issuecomment-3717222957) recently, too.

![CleanShot 2026-01-18 at 12.43.27@2x.png](https://images.danmatthews.me/images/4e30ce7d-3e70-42ad-bf36-947860a19f07.png)

More will follow.

## So, what can we do?

**Risk assess heavily** - this is something I don’t think enough companies are doing right now. If you’re planning on firing, or skipping hiring, because you can rely on AI to pick up the slack, what happens when AI costs **double**, **triple**, or those tools become unavailable suddenly? or they only come availble by agreeing to a particular terms of service.

**Beware the grift,** and try not to let people on YouTube make you feel like you’re missing out, or going to be made obsolete if you can’t somehow run 3 different LLMs and twenty sub-agents. A good measure of this is - are they selling something? or talking to someone who’s selling something? then it’s probably a good idea to take their advice and findings with a pinch of salt. Actively oppose people who you think are proliferating this message for their own bottom line.

**Be mindful of the capitalist component where possible** - if you’re questioning why a certain company is leaning heavily into LLMs in their product for no apparent reason - check who funds that company, and the other companies they’re invested in. Be more cautious with who you give your money to.

**Use these tools as an augmentation, not a replacement** - use them to automate away tasks that suck, places where human error can slip in, where fatigue can mean that people miss things where machines are unlikely to. Trust, but verify their outputs.

**Make an active effort not neccessarily to rely on them less, but to understand the output where possible** - ask Claude for a solution to a bug, because YES, it is better than googling it -  but make sure you understand WHY that bug occured, what limitation of the language or system caused it, and internalise that if you can. Where you have LLMs writing tests, make sure you understand WHAT it’s testing.

**Encourage the next generation** - If LLMs truly will give us more time to do other things in our jobs rather than writing code, then I proffer that we should be spending this time offering mentorship and placements for Junior devs where we can, and encouraging them to learn fundamentals before learning prompt engineering. Not to stop them using LLMs, but to ensure they can harness the outputs from LLMs in safe manner, and indentify issues that could cause real world problems.
